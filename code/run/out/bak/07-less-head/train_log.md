# Network hyperparmeters
```python
dim_model = 32
dim_ffn = 256
dim_k = dim_v = 32
num_layers = 3
num_heads = 2
```

# Start training at 2022-04-05 13:59:40

```python
batch_size = 12
max_num_samples = 1000
num_epochs = 500
learning_rate = 1.00e-03
dropout_prob = 0.00000
weight_decay = 0.00000
```

- Epoch 01, Iteration 000001, Loss 0.254408
- Epoch 01, Iteration 000051, Loss 0.132258
- Epoch 01, Iteration 000101, Loss 0.126650
- Epoch 01, Iteration 000151, Loss 0.106523
- Epoch 01, Iteration 000201, Loss 0.098012
- Epoch 01, Iteration 000251, Loss 0.103573
- Epoch 01, Iteration 000301, Loss 0.083680
- Epoch 01, Iteration 000351, Loss 0.076474
- Epoch 01, Iteration 000401, Loss 0.082205
- Epoch 01, Iteration 000451, Loss 0.088987
- Epoch 01, Iteration 000501, Loss 0.092384
- Epoch 02, Iteration 000551, Loss 0.080799
- Epoch 02, Iteration 000601, Loss 0.071480
- Epoch 02, Iteration 000651, Loss 0.068379
- Epoch 02, Iteration 000701, Loss 0.081237
- Epoch 02, Iteration 000751, Loss 0.069432
- Epoch 02, Iteration 000801, Loss 0.072278
- Epoch 02, Iteration 000851, Loss 0.078770
- Epoch 02, Iteration 000901, Loss 0.069691
- Epoch 02, Iteration 000951, Loss 0.076239
- Epoch 02, Iteration 001001, Loss 0.067577
- Epoch 02, Iteration 001051, Loss 0.065387
- Epoch 03, Iteration 001101, Loss 0.068949
- Epoch 03, Iteration 001151, Loss 0.066223
- Epoch 03, Iteration 001201, Loss 0.069021
- Epoch 03, Iteration 001251, Loss 0.072097
- Epoch 03, Iteration 001301, Loss 0.064169
- Epoch 03, Iteration 001351, Loss 0.071924
- Epoch 03, Iteration 001401, Loss 0.070914
- Epoch 03, Iteration 001451, Loss 0.070613
- Epoch 03, Iteration 001501, Loss 0.080178
- Epoch 03, Iteration 001551, Loss 0.065195
- Epoch 03, Iteration 001601, Loss 0.076076
- Epoch 04, Iteration 001651, Loss 0.069068
- Epoch 04, Iteration 001701, Loss 0.065493
- Epoch 04, Iteration 001751, Loss 0.064476
- Epoch 04, Iteration 001801, Loss 0.062892
- Epoch 04, Iteration 001851, Loss 0.063274
- Epoch 04, Iteration 001901, Loss 0.061273
- Epoch 04, Iteration 001951, Loss 0.055592
- Epoch 04, Iteration 002001, Loss 0.062479
- Epoch 04, Iteration 002051, Loss 0.063386
- Epoch 04, Iteration 002101, Loss 0.065101
- Epoch 05, Iteration 002151, Loss 0.071949
- Epoch 05, Iteration 002201, Loss 0.063884
- Epoch 05, Iteration 002251, Loss 0.066813
- Epoch 05, Iteration 002301, Loss 0.060289
- Epoch 05, Iteration 002351, Loss 0.077635
- Epoch 05, Iteration 002401, Loss 0.064065
# Restart training at 2022-04-05 14:16:04

```python
batch_size = 12
max_num_samples = 1000
num_epochs = 500
learning_rate = 1.00e-04
dropout_prob = 0.00000
weight_decay = 0.00000
```

- Epoch 01, Iteration 000001, Loss 0.067482
- Epoch 01, Iteration 000051, Loss 0.058746
- Epoch 01, Iteration 000101, Loss 0.055879
- Epoch 01, Iteration 000151, Loss 0.055460
- Epoch 01, Iteration 000201, Loss 0.057683
- Epoch 01, Iteration 000251, Loss 0.055465
- Epoch 01, Iteration 000301, Loss 0.058093
- Epoch 01, Iteration 000351, Loss 0.058130
- Epoch 01, Iteration 000401, Loss 0.057235
- Epoch 01, Iteration 000451, Loss 0.061851
- Epoch 01, Iteration 000501, Loss 0.063778
- Epoch 02, Iteration 000551, Loss 0.057532
- Epoch 02, Iteration 000601, Loss 0.061150
- Epoch 02, Iteration 000651, Loss 0.062510
- Epoch 02, Iteration 000701, Loss 0.058033
- Epoch 02, Iteration 000751, Loss 0.057291
- Epoch 02, Iteration 000801, Loss 0.065011
- Epoch 02, Iteration 000851, Loss 0.059292
- Epoch 02, Iteration 000901, Loss 0.061072
- Epoch 02, Iteration 000951, Loss 0.061630
- Epoch 02, Iteration 001001, Loss 0.058636
- Epoch 02, Iteration 001051, Loss 0.055633
