# Network hyperparmeters
```python
dim_model = 32
dim_ffn = 256
dim_k = dim_v = 32
num_layers = 6
num_heads = 4
```

# Start training at 2022-04-05 07:00:47

```python
batch_size = 4
max_num_samples = 1000
num_epochs = 500
learning_rate = 1.00e-03
dropout_prob = 0.00000
weight_decay = 0.00000
```

- Epoch 01, Iteration 000001, Loss 0.274179
- Epoch 01, Iteration 000051, Loss 0.118843
- Epoch 01, Iteration 000101, Loss 0.148175
- Epoch 01, Iteration 000151, Loss 0.158078
- Epoch 01, Iteration 000201, Loss 0.109761
- Epoch 01, Iteration 000251, Loss 0.144993
- Epoch 01, Iteration 000301, Loss 0.183514
- Epoch 01, Iteration 000351, Loss 0.106879
- Epoch 01, Iteration 000401, Loss 0.109093
- Epoch 01, Iteration 000451, Loss 0.120864
- Epoch 01, Iteration 000501, Loss 0.148802
- Epoch 01, Iteration 000551, Loss 0.136813
- Epoch 01, Iteration 000601, Loss 0.120293
- Epoch 01, Iteration 000651, Loss 0.145886
- Epoch 01, Iteration 000701, Loss 0.156233
- Epoch 01, Iteration 000751, Loss 0.124544
- Epoch 01, Iteration 000801, Loss 0.150731
- Epoch 01, Iteration 000851, Loss 0.114343
- Epoch 01, Iteration 000901, Loss 0.144612
- Epoch 01, Iteration 000951, Loss 0.108083
- Epoch 01, Iteration 001001, Loss 0.165605
- Epoch 01, Iteration 001051, Loss 0.100129
- Epoch 01, Iteration 001101, Loss 0.102143
- Epoch 01, Iteration 001151, Loss 0.099718
- Epoch 01, Iteration 001201, Loss 0.085649
- Epoch 01, Iteration 001251, Loss 0.086114
- Epoch 01, Iteration 001301, Loss 0.122749
- Epoch 01, Iteration 001351, Loss 0.097137
- Epoch 01, Iteration 001401, Loss 0.101852
- Epoch 01, Iteration 001451, Loss 0.112896
- Epoch 01, Iteration 001501, Loss 0.081547
- Epoch 01, Iteration 001551, Loss 0.116982
- Epoch 01, Iteration 001601, Loss 0.100644
- Epoch 02, Iteration 001651, Loss 0.092392
- Epoch 02, Iteration 001701, Loss 0.100815
- Epoch 02, Iteration 001751, Loss 0.077426
- Epoch 02, Iteration 001801, Loss 0.076777
- Epoch 02, Iteration 001851, Loss 0.085595
- Epoch 02, Iteration 001901, Loss 0.081500
- Epoch 02, Iteration 001951, Loss 0.078224
- Epoch 02, Iteration 002001, Loss 0.079636
- Epoch 02, Iteration 002051, Loss 0.081851
- Epoch 02, Iteration 002101, Loss 0.120214
- Epoch 02, Iteration 002151, Loss 0.077897
- Epoch 02, Iteration 002201, Loss 0.067680
- Epoch 02, Iteration 002251, Loss 0.060460
- Epoch 02, Iteration 002301, Loss 0.063307
- Epoch 02, Iteration 002351, Loss 0.063198
- Epoch 02, Iteration 002401, Loss 0.064394
- Epoch 02, Iteration 002451, Loss 0.057249
- Epoch 02, Iteration 002501, Loss 0.070481
- Epoch 02, Iteration 002551, Loss 0.066690
- Epoch 02, Iteration 002601, Loss 0.064486
- Epoch 02, Iteration 002651, Loss 0.072278
- Epoch 02, Iteration 002701, Loss 0.046480
- Epoch 02, Iteration 002751, Loss 0.053868
- Epoch 02, Iteration 002801, Loss 0.054044
- Epoch 02, Iteration 002851, Loss 0.054499
- Epoch 02, Iteration 002901, Loss 0.060558
- Epoch 02, Iteration 002951, Loss 0.067605
- Epoch 02, Iteration 003001, Loss 0.051755
- Epoch 02, Iteration 003051, Loss 0.050131
- Epoch 02, Iteration 003101, Loss 0.054864
- Epoch 02, Iteration 003151, Loss 0.054217
- Epoch 02, Iteration 003201, Loss 0.041488
- Epoch 03, Iteration 003251, Loss 0.057229
- Epoch 03, Iteration 003301, Loss 0.054269
- Epoch 03, Iteration 003351, Loss 0.045037
- Epoch 03, Iteration 003401, Loss 0.055989
- Epoch 03, Iteration 003451, Loss 0.054408
- Epoch 03, Iteration 003501, Loss 0.057944
- Epoch 03, Iteration 003551, Loss 0.067882
- Epoch 03, Iteration 003601, Loss 0.046210
- Epoch 03, Iteration 003651, Loss 0.050871
- Epoch 03, Iteration 003701, Loss 0.064900
- Epoch 03, Iteration 003751, Loss 0.063399
- Epoch 03, Iteration 003801, Loss 0.044063
- Epoch 03, Iteration 003851, Loss 0.050562
- Epoch 03, Iteration 003901, Loss 0.050139
- Epoch 03, Iteration 003951, Loss 0.046214
- Epoch 03, Iteration 004001, Loss 0.044697
- Epoch 03, Iteration 004051, Loss 0.048790
- Epoch 03, Iteration 004101, Loss 0.042665
- Epoch 03, Iteration 004151, Loss 0.045868
- Epoch 03, Iteration 004201, Loss 0.041618
- Epoch 03, Iteration 004251, Loss 0.041430
- Epoch 03, Iteration 004301, Loss 0.045425
- Epoch 03, Iteration 004351, Loss 0.029762
- Epoch 03, Iteration 004401, Loss 0.036180
- Epoch 03, Iteration 004451, Loss 0.059495
- Epoch 03, Iteration 004501, Loss 0.045443
- Epoch 03, Iteration 004551, Loss 0.039031
- Epoch 03, Iteration 004601, Loss 0.042469
- Epoch 03, Iteration 004651, Loss 0.039608
- Epoch 03, Iteration 004701, Loss 0.055049
- Epoch 03, Iteration 004751, Loss 0.040086
- Epoch 03, Iteration 004801, Loss 0.049564
- Epoch 04, Iteration 004851, Loss 0.031687
- Epoch 04, Iteration 004901, Loss 0.039070
- Epoch 04, Iteration 004951, Loss 0.034781
- Epoch 04, Iteration 005001, Loss 0.025789
- Epoch 04, Iteration 005051, Loss 0.038584
- Epoch 04, Iteration 005101, Loss 0.043430
- Epoch 04, Iteration 005151, Loss 0.035609
- Epoch 04, Iteration 005201, Loss 0.049015
- Epoch 04, Iteration 005251, Loss 0.036683
- Epoch 04, Iteration 005301, Loss 0.038902
- Epoch 04, Iteration 005351, Loss 0.036440
- Epoch 04, Iteration 005401, Loss 0.032546
- Epoch 04, Iteration 005451, Loss 0.044775
- Epoch 04, Iteration 005501, Loss 0.044326
- Epoch 04, Iteration 005551, Loss 0.024668
- Epoch 04, Iteration 005601, Loss 0.027350
- Epoch 04, Iteration 005651, Loss 0.033202
- Epoch 04, Iteration 005701, Loss 0.032934
- Epoch 04, Iteration 005751, Loss 0.034902
- Epoch 04, Iteration 005801, Loss 0.033387
- Epoch 04, Iteration 005851, Loss 0.037291
- Epoch 04, Iteration 005901, Loss 0.043725
- Epoch 04, Iteration 005951, Loss 0.023541
- Epoch 04, Iteration 006001, Loss 0.047399
- Epoch 04, Iteration 006051, Loss 0.033686
- Epoch 04, Iteration 006101, Loss 0.031977
- Epoch 04, Iteration 006151, Loss 0.053061
- Epoch 04, Iteration 006201, Loss 0.045194
- Epoch 04, Iteration 006251, Loss 0.033568
- Epoch 04, Iteration 006301, Loss 0.032902
- Epoch 04, Iteration 006351, Loss 0.033870
- Epoch 04, Iteration 006401, Loss 0.025519
- Epoch 05, Iteration 006451, Loss 0.030657
- Epoch 05, Iteration 006501, Loss 0.039698
- Epoch 05, Iteration 006551, Loss 0.030098
- Epoch 05, Iteration 006601, Loss 0.024488
- Epoch 05, Iteration 006651, Loss 0.039508
- Epoch 05, Iteration 006701, Loss 0.027207
- Epoch 05, Iteration 006751, Loss 0.024362
- Epoch 05, Iteration 006801, Loss 0.036618
- Epoch 05, Iteration 006851, Loss 0.047574
- Epoch 05, Iteration 006901, Loss 0.026596
- Epoch 05, Iteration 006951, Loss 0.039429
- Epoch 05, Iteration 007001, Loss 0.034229
- Epoch 05, Iteration 007051, Loss 0.036051
- Epoch 05, Iteration 007101, Loss 0.029444
- Epoch 05, Iteration 007151, Loss 0.024316
- Epoch 05, Iteration 007201, Loss 0.037549
- Epoch 05, Iteration 007251, Loss 0.035698
- Epoch 05, Iteration 007301, Loss 0.026232
- Epoch 05, Iteration 007351, Loss 0.027574
- Epoch 05, Iteration 007401, Loss 0.020455
- Epoch 05, Iteration 007451, Loss 0.045681
- Epoch 05, Iteration 007501, Loss 0.021504
- Epoch 05, Iteration 007551, Loss 0.047392
- Epoch 05, Iteration 007601, Loss 0.035380
- Epoch 05, Iteration 007651, Loss 0.027379
- Epoch 05, Iteration 007701, Loss 0.028551
- Epoch 05, Iteration 007751, Loss 0.033079
- Epoch 05, Iteration 007801, Loss 0.028102
- Epoch 05, Iteration 007851, Loss 0.039087
- Epoch 05, Iteration 007901, Loss 0.032420
- Epoch 05, Iteration 007951, Loss 0.033617
- Epoch 05, Iteration 008001, Loss 0.026107
- Epoch 06, Iteration 008051, Loss 0.021490
- Epoch 06, Iteration 008101, Loss 0.022305
- Epoch 06, Iteration 008151, Loss 0.031444
- Epoch 06, Iteration 008201, Loss 0.031915
- Epoch 06, Iteration 008251, Loss 0.024403
- Epoch 06, Iteration 008301, Loss 0.016806
- Epoch 06, Iteration 008351, Loss 0.022199
- Epoch 06, Iteration 008401, Loss 0.024088
- Epoch 06, Iteration 008451, Loss 0.025409
- Epoch 06, Iteration 008501, Loss 0.030407
- Epoch 06, Iteration 008551, Loss 0.025571
- Epoch 06, Iteration 008601, Loss 0.028962
- Epoch 06, Iteration 008651, Loss 0.025615
- Epoch 06, Iteration 008701, Loss 0.021244
- Epoch 06, Iteration 008751, Loss 0.017181
- Epoch 06, Iteration 008801, Loss 0.031786
- Epoch 06, Iteration 008851, Loss 0.023787
- Epoch 06, Iteration 008901, Loss 0.031337
- Epoch 06, Iteration 008951, Loss 0.031889
- Epoch 06, Iteration 009001, Loss 0.017109
- Epoch 06, Iteration 009051, Loss 0.039435
- Epoch 06, Iteration 009101, Loss 0.035519
- Epoch 06, Iteration 009151, Loss 0.021430
- Epoch 06, Iteration 009201, Loss 0.024777
- Epoch 06, Iteration 009251, Loss 0.019348
- Epoch 06, Iteration 009301, Loss 0.020519
- Epoch 06, Iteration 009351, Loss 0.021670
- Epoch 06, Iteration 009401, Loss 0.021520
- Epoch 06, Iteration 009451, Loss 0.016736
- Epoch 06, Iteration 009501, Loss 0.018748
- Epoch 06, Iteration 009551, Loss 0.026108
- Epoch 06, Iteration 009601, Loss 0.019709
- Epoch 07, Iteration 009651, Loss 0.023112
- Epoch 07, Iteration 009701, Loss 0.028740
- Epoch 07, Iteration 009751, Loss 0.016848
- Epoch 07, Iteration 009801, Loss 0.027743
- Epoch 07, Iteration 009851, Loss 0.015700
- Epoch 07, Iteration 009901, Loss 0.025621
- Epoch 07, Iteration 009951, Loss 0.021943
- Epoch 07, Iteration 010001, Loss 0.030671
- Epoch 07, Iteration 010051, Loss 0.012917
- Epoch 07, Iteration 010101, Loss 0.027464
# Restart training at 2022-04-05 08:08:27

```python
batch_size = 4
max_num_samples = 1000
num_epochs = 500
learning_rate = 1.00e-04
dropout_prob = 0.00000
weight_decay = 0.00000
```

- Epoch 01, Iteration 000001, Loss 0.024514
- Epoch 01, Iteration 000051, Loss 0.009592
- Epoch 01, Iteration 000101, Loss 0.022790
- Epoch 01, Iteration 000151, Loss 0.018432
- Epoch 01, Iteration 000201, Loss 0.012818
- Epoch 01, Iteration 000251, Loss 0.016324
- Epoch 01, Iteration 000301, Loss 0.014571
- Epoch 01, Iteration 000351, Loss 0.012856
- Epoch 01, Iteration 000401, Loss 0.008765
- Epoch 01, Iteration 000451, Loss 0.011884
- Epoch 01, Iteration 000501, Loss 0.013975
- Epoch 01, Iteration 000551, Loss 0.016538
- Epoch 01, Iteration 000601, Loss 0.016335
- Epoch 01, Iteration 000651, Loss 0.013870
- Epoch 01, Iteration 000701, Loss 0.010002
- Epoch 01, Iteration 000751, Loss 0.016721
- Epoch 01, Iteration 000801, Loss 0.017619
- Epoch 01, Iteration 000851, Loss 0.007967
- Epoch 01, Iteration 000901, Loss 0.011633
- Epoch 01, Iteration 000951, Loss 0.020349
- Epoch 01, Iteration 001001, Loss 0.010873
- Epoch 01, Iteration 001051, Loss 0.011190
- Epoch 01, Iteration 001101, Loss 0.015334
- Epoch 01, Iteration 001151, Loss 0.011208
- Epoch 01, Iteration 001201, Loss 0.010628
- Epoch 01, Iteration 001251, Loss 0.009535
- Epoch 01, Iteration 001301, Loss 0.018832
- Epoch 01, Iteration 001351, Loss 0.013041
- Epoch 01, Iteration 001401, Loss 0.013212
- Epoch 01, Iteration 001451, Loss 0.010384
- Epoch 01, Iteration 001501, Loss 0.014099
- Epoch 01, Iteration 001551, Loss 0.007919
- Epoch 01, Iteration 001601, Loss 0.012646
- Epoch 02, Iteration 001651, Loss 0.009975
- Epoch 02, Iteration 001701, Loss 0.017237
- Epoch 02, Iteration 001751, Loss 0.008297
- Epoch 02, Iteration 001801, Loss 0.019797
- Epoch 02, Iteration 001851, Loss 0.014521
- Epoch 02, Iteration 001901, Loss 0.011205
- Epoch 02, Iteration 001951, Loss 0.006694
- Epoch 02, Iteration 002001, Loss 0.013006
- Epoch 02, Iteration 002051, Loss 0.011906
- Epoch 02, Iteration 002101, Loss 0.013479
- Epoch 02, Iteration 002151, Loss 0.014457
- Epoch 02, Iteration 002201, Loss 0.027488
- Epoch 02, Iteration 002251, Loss 0.012675
- Epoch 02, Iteration 002301, Loss 0.006596
- Epoch 02, Iteration 002351, Loss 0.007792
- Epoch 02, Iteration 002401, Loss 0.009577
- Epoch 02, Iteration 002451, Loss 0.013324
- Epoch 02, Iteration 002501, Loss 0.013812
- Epoch 02, Iteration 002551, Loss 0.010047
- Epoch 02, Iteration 002601, Loss 0.011576
- Epoch 02, Iteration 002651, Loss 0.009803
- Epoch 02, Iteration 002701, Loss 0.009250
- Epoch 02, Iteration 002751, Loss 0.012742
- Epoch 02, Iteration 002801, Loss 0.007342
- Epoch 02, Iteration 002851, Loss 0.006130
- Epoch 02, Iteration 002901, Loss 0.008990
- Epoch 02, Iteration 002951, Loss 0.013981
- Epoch 02, Iteration 003001, Loss 0.005346
- Epoch 02, Iteration 003051, Loss 0.006588
- Epoch 02, Iteration 003101, Loss 0.017125
- Epoch 02, Iteration 003151, Loss 0.013497
- Epoch 02, Iteration 003201, Loss 0.013940
- Epoch 03, Iteration 003251, Loss 0.013760
- Epoch 03, Iteration 003301, Loss 0.009863
- Epoch 03, Iteration 003351, Loss 0.008359
- Epoch 03, Iteration 003401, Loss 0.013298
- Epoch 03, Iteration 003451, Loss 0.012819
- Epoch 03, Iteration 003501, Loss 0.010310
- Epoch 03, Iteration 003551, Loss 0.007738
- Epoch 03, Iteration 003601, Loss 0.012885
- Epoch 03, Iteration 003651, Loss 0.010089
- Epoch 03, Iteration 003701, Loss 0.011778
- Epoch 03, Iteration 003751, Loss 0.009154
- Epoch 03, Iteration 003801, Loss 0.010602
- Epoch 03, Iteration 003851, Loss 0.010774
- Epoch 03, Iteration 003901, Loss 0.006544
- Epoch 03, Iteration 003951, Loss 0.007485
- Epoch 03, Iteration 004001, Loss 0.009137
- Epoch 03, Iteration 004051, Loss 0.008929
- Epoch 03, Iteration 004101, Loss 0.008491
- Epoch 03, Iteration 004151, Loss 0.005935
- Epoch 03, Iteration 004201, Loss 0.009789
- Epoch 03, Iteration 004251, Loss 0.008904
- Epoch 03, Iteration 004301, Loss 0.011441
- Epoch 03, Iteration 004351, Loss 0.011006
- Epoch 03, Iteration 004401, Loss 0.008273
- Epoch 03, Iteration 004451, Loss 0.011176
- Epoch 03, Iteration 004501, Loss 0.012086
- Epoch 03, Iteration 004551, Loss 0.006420
- Epoch 03, Iteration 004601, Loss 0.009773
- Epoch 03, Iteration 004651, Loss 0.009163
- Epoch 03, Iteration 004701, Loss 0.012041
- Epoch 03, Iteration 004751, Loss 0.010102
- Epoch 03, Iteration 004801, Loss 0.007186
- Epoch 04, Iteration 004851, Loss 0.018575
- Epoch 04, Iteration 004901, Loss 0.008196
- Epoch 04, Iteration 004951, Loss 0.011167
- Epoch 04, Iteration 005001, Loss 0.006952
- Epoch 04, Iteration 005051, Loss 0.007629
- Epoch 04, Iteration 005101, Loss 0.010055
- Epoch 04, Iteration 005151, Loss 0.007331
- Epoch 04, Iteration 005201, Loss 0.014150
- Epoch 04, Iteration 005251, Loss 0.006773
- Epoch 04, Iteration 005301, Loss 0.007514
- Epoch 04, Iteration 005351, Loss 0.012586
- Epoch 04, Iteration 005401, Loss 0.018091
- Epoch 04, Iteration 005451, Loss 0.013066
- Epoch 04, Iteration 005501, Loss 0.016031
- Epoch 04, Iteration 005551, Loss 0.009088
- Epoch 04, Iteration 005601, Loss 0.004591
- Epoch 04, Iteration 005651, Loss 0.006163
- Epoch 04, Iteration 005701, Loss 0.010457
- Epoch 04, Iteration 005751, Loss 0.007695
- Epoch 04, Iteration 005801, Loss 0.015797
- Epoch 04, Iteration 005851, Loss 0.011574
- Epoch 04, Iteration 005901, Loss 0.007605
- Epoch 04, Iteration 005951, Loss 0.009145
- Epoch 04, Iteration 006001, Loss 0.008871
- Epoch 04, Iteration 006051, Loss 0.009210
- Epoch 04, Iteration 006101, Loss 0.010839
